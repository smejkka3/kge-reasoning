{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-26-ee64e244a718>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-ee64e244a718>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def generate_candidates(X, strategy, target_rel=None, max_candidates, consolidate_sides=False, seed=0):\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def generate_candidates(X, strategy, target_rel, max_candidates, consolidate_sides=False, seed=0):\n",
    "    \"\"\" Generate candidate statements from an existing knowledge graph using a defined strategy.\n",
    "        Parameters\n",
    "        ----------\n",
    "        strategy: string\n",
    "            The candidates generation strategy.\n",
    "            - 'random_uniform' : generates N candidates (N <= max_candidates) based on a uniform random sampling of\n",
    "                head and tail entities.\n",
    "            - 'entity_frequency' : generates candidates by sampling entities with low frequency.\n",
    "            - 'graph_degree' : generates candidates by sampling entities with a low graph degree.\n",
    "            - 'cluster_coefficient' : generates candidates by sampling entities with a low clustering coefficient.\n",
    "            - 'cluster_triangles' : generates candidates by sampling entities with a low number of cluster triangles.\n",
    "            - 'cluster_squares' : generates candidates by sampling entities with a low number of cluster squares.\n",
    "        max_candidates: int or float\n",
    "            The maximum numbers of candidates generated by 'strategy'.\n",
    "            Can be an absolute number or a percentage [0,1].\n",
    "            This does not guarantee the number of candidates generated.\n",
    "        target_rel : str\n",
    "            Target relation to focus on. The function will generate candidate\n",
    "             statements only with this specific relation type.\n",
    "        consolidate_sides: bool\n",
    "            If True will generate candidate statements as a product of\n",
    "            unique head and tail entities, otherwise will\n",
    "            consider head and tail entities separately. Default: False.\n",
    "        seed : int\n",
    "            Seed to use for reproducible results.\n",
    "        Returns\n",
    "        -------\n",
    "        X_candidates : ndarray, shape [n, 3]\n",
    "            A list of candidate statements.\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import numpy as np\n",
    "        >>> from ampligraph.discovery.discovery import generate_candidates\n",
    "        >>>\n",
    "        >>> X = np.array([['a', 'y', 'b'],\n",
    "        >>>               ['b', 'y', 'a'],\n",
    "        >>>               ['a', 'y', 'c'],\n",
    "        >>>               ['c', 'y', 'a'],\n",
    "        >>>               ['a', 'y', 'd'],\n",
    "        >>>               ['c', 'y', 'd'],\n",
    "        >>>               ['b', 'y', 'c'],\n",
    "        >>>               ['f', 'y', 'e']])\n",
    "        >>> X_candidates = generate_candidates(X, strategy='graph_degree', target_rel='y', max_candidates=3)\n",
    "        >>> ([['a', 'y', 'e'],\n",
    "        >>>  ['f', 'y', 'a'],\n",
    "        >>>  ['c', 'y', 'e']])\n",
    "    \"\"\"\n",
    "\n",
    "    if strategy not in ['random_uniform', 'entity_frequency',\n",
    "                        'graph_degree', 'cluster_coefficient',\n",
    "                        'cluster_triangles', 'cluster_squares']:\n",
    "        msg = '%s is not a valid candidate generation strategy.' % strategy\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if target_rel not in np.unique(X[:, 1]):\n",
    "        # No error as may be case where target_rel is not in X\n",
    "        msg = 'Target relation is not found in triples.'\n",
    "        logger.warning(msg)\n",
    "\n",
    "    if not isinstance(max_candidates, (float, int)):\n",
    "        msg = 'Parameter max_candidates must be a float or int.'\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if max_candidates <= 0:\n",
    "        msg = 'Parameter max_candidates must be a positive integer ' \\\n",
    "              'or float in range (0,1].'\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if isinstance(max_candidates, float):\n",
    "        max_candidates = int(max_candidates * len(X))\n",
    "\n",
    "    def _filter_candidates(X_candidates, X, remove_reflexive=True):\n",
    "        \"\"\" Inner function to filter candidate statements from X_candidates that are in X.\n",
    "        \"\"\"\n",
    "        X_candidates = _setdiff2d(X_candidates, X)\n",
    "        # Filter statements that are ['x', rel, 'x']\n",
    "        if remove_reflexive:\n",
    "            keep_idx = np.where(X_candidates[:, 0] != X_candidates[:, 2])\n",
    "            X_candidates = X_candidates[keep_idx]\n",
    "\n",
    "        return X_candidates\n",
    "\n",
    "    # Set random seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Get entities linked with this relation\n",
    "    if consolidate_sides:\n",
    "        e_s = np.unique(np.concatenate((X[:, 0], X[:, 2])))\n",
    "        e_o = e_s\n",
    "    else:\n",
    "        e_s = np.unique(X[:, 0])\n",
    "        e_o = np.unique(X[:, 2])\n",
    "\n",
    "    logger.info('Generating candidates using {} strategy.'.format(strategy))\n",
    "\n",
    "    if strategy == 'random_uniform':\n",
    "\n",
    "        # Take close to sqrt of max_candidates so that: len(meshgrid result) == max_candidates\n",
    "        sample_size = int(np.sqrt(max_candidates) + 10)  # +10 to allow for reduction in sampled array due to filtering\n",
    "\n",
    "        X_candidates = np.zeros([max_candidates, 3], dtype=object)  # Pre-allocate X_candidates array\n",
    "        num_retries, max_retries = 0, 5  # Retry up to 5 times to reach max_candidates\n",
    "        start_idx, end_idx = 0, 0  #\n",
    "\n",
    "        while end_idx <= max_candidates - 1:\n",
    "            sample_e_s = np.random.choice(e_s, size=sample_size, replace=False)\n",
    "            sample_e_o = np.random.choice(e_o, size=sample_size, replace=False)\n",
    "\n",
    "            gen_candidates = np.array(np.meshgrid(sample_e_s, target_rel, sample_e_o)).T.reshape(-1, 3)\n",
    "            gen_candidates = _filter_candidates(gen_candidates, X)\n",
    "\n",
    "            # Select either all of gen_candidates or just enough to fill X_candidates\n",
    "            select_idx = min(len(gen_candidates), len(X_candidates) - start_idx)\n",
    "            end_idx = start_idx + select_idx\n",
    "\n",
    "            X_candidates[start_idx:end_idx, :] = gen_candidates[0:select_idx, :]\n",
    "            start_idx = end_idx\n",
    "\n",
    "            num_retries += 1\n",
    "            if num_retries == max_retries:\n",
    "                break\n",
    "\n",
    "        # end_idx will equal max_candidates in most cases, but could be less\n",
    "        return X_candidates[0:end_idx, :]\n",
    "\n",
    "    elif strategy == 'entity_frequency':\n",
    "\n",
    "        # Get entity counts and sort them in ascending order\n",
    "        if consolidate_sides:\n",
    "            e_s_counts = np.array(np.unique(X[:, [0, 2]], return_counts=True)).T\n",
    "            e_o_counts = e_s_counts\n",
    "        else:\n",
    "            e_s_counts = np.array(np.unique(X[:, 0], return_counts=True)).T\n",
    "            e_o_counts = np.array(np.unique(X[:, 2], return_counts=True)).T\n",
    "\n",
    "        e_s_weights = e_s_counts[:, 1].astype(np.float64) / np.sum(e_s_counts[:, 1].astype(np.float64))\n",
    "        e_o_weights = e_o_counts[:, 1].astype(np.float64) / np.sum(e_o_counts[:, 1].astype(np.float64))\n",
    "\n",
    "    elif strategy in ['graph_degree', 'cluster_coefficient', 'cluster_triangles', 'cluster_squares']:\n",
    "\n",
    "        # Create networkx graph\n",
    "        G = nx.Graph()\n",
    "        for row in X:\n",
    "            G.add_nodes_from([row[0], row[2]])\n",
    "            G.add_edge(row[0], row[2], name=row[1])\n",
    "\n",
    "        # Calculate node metrics\n",
    "        if strategy == 'graph_degree':\n",
    "            C = {i: j for i, j in G.degree()}\n",
    "        elif strategy == 'cluster_coefficient':\n",
    "            C = nx.algorithms.cluster.clustering(G)\n",
    "        elif strategy == 'cluster_triangles':\n",
    "            C = nx.algorithms.cluster.triangles(G)\n",
    "        elif strategy == 'cluster_squares':\n",
    "            C = nx.algorithms.cluster.square_clustering(G)\n",
    "\n",
    "        e_s_weights = np.array([C[x] for x in e_s], dtype=np.float64)\n",
    "        e_o_weights = np.array([C[x] for x in e_o], dtype=np.float64)\n",
    "\n",
    "        e_s_weights = e_s_weights / np.sum(e_s_weights)\n",
    "        e_o_weights = e_o_weights / np.sum(e_o_weights)\n",
    "\n",
    "    # Take close to sqrt of max_candidates so that: len(meshgrid result) == max_candidates\n",
    "    sample_size = int(np.sqrt(max_candidates) + 10)  # +10 to allow for reduction in sampled array due to filtering\n",
    "\n",
    "    X_candidates = np.zeros([max_candidates, 3], dtype=object)  # Pre-allocate X_candidates array\n",
    "    num_retries, max_retries = 0, 5  # Retry up to 5 times to reach max_candidates\n",
    "    start_idx, end_idx = 0, 0\n",
    "\n",
    "    while end_idx <= max_candidates - 1:\n",
    "\n",
    "        sample_e_s = np.random.choice(e_s, size=sample_size, replace=True, p=e_s_weights)\n",
    "        sample_e_o = np.random.choice(e_o, size=sample_size, replace=True, p=e_o_weights)\n",
    "\n",
    "        gen_candidates = np.array(np.meshgrid(sample_e_s, target_rel, sample_e_o)).T.reshape(-1, 3)\n",
    "        gen_candidates = _filter_candidates(gen_candidates, X)\n",
    "\n",
    "        # Select either all of gen_candidates or just enough to fill X_candidates\n",
    "        select_idx = min(len(gen_candidates), len(X_candidates) - start_idx)\n",
    "        end_idx = start_idx + select_idx\n",
    "\n",
    "        X_candidates[start_idx:end_idx, :] = gen_candidates[0:select_idx, :]\n",
    "        start_idx = end_idx\n",
    "\n",
    "        num_retries += 1\n",
    "\n",
    "        if num_retries == max_retries:\n",
    "            break\n",
    "\n",
    "    # end_idx will be max_candidates in most cases, but could be less\n",
    "    return X_candidates[0:end_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target relation is not found in triples.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import networkx as nx\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "X = np.array([['0', '0', '1'], \\\n",
    "            ['1', '0', '2'], \\\n",
    "            ['3', '1', '4'], \\\n",
    "            ['3', '2', '0'], \\\n",
    "            ['5', '3', '6'], \\\n",
    "            ['5', '4', '6'], \\\n",
    "            ['7', '5', '8'], \\\n",
    "            ['9', '6', '10']])\n",
    "X_candidates = generate_candidates(X, strategy='graph_degree', target_rel=None, max_candidates=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['3', None, '10'],\n",
       "       ['5', None, '10'],\n",
       "       ['3', None, '10']], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DBSCAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-001db821ffaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mfind_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclustering_algorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"entity\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[1;32m     22\u001b[0m     \u001b[0mPerform\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbased\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0manalysis\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0mknowledge\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DBSCAN' is not defined"
     ]
    }
   ],
   "source": [
    "def _setdiff2d(A, B):\n",
    "    \"\"\" Utility function equivalent to numpy.setdiff1d on 2d arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : ndarray, shape [n, m]\n",
    "    B : ndarray, shape [n, m]\n",
    "    Returns\n",
    "    -------\n",
    "    np.array, shape [k, m]\n",
    "        Rows of A that are not in B.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(A.shape) != 2 or len(B.shape) != 2:\n",
    "        raise RuntimeError('Input arrays must be 2-dimensional.')\n",
    "\n",
    "    tmp = np.prod(np.swapaxes(A[:, :, None], 1, 2) == B, axis=2)\n",
    "    return A[~ np.sum(np.cumsum(tmp, axis=0) * tmp == 1, axis=1).astype(bool)]\n",
    "\n",
    "\n",
    "def find_clusters(X, model, clustering_algorithm=DBSCAN(), mode=\"entity\"):\n",
    "    \"\"\"\n",
    "    Perform link-based cluster analysis on a knowledge graph.\n",
    "    The clustering happens on the embedding space of the entities and relations.\n",
    "    For example, if we cluster some entities of a model that uses `k=100` (i.e. embedding space of size 100),\n",
    "    we will apply the chosen clustering algorithm on the 100-dimensional space of the provided input samples.\n",
    "    Clustering can be used to evaluate the quality of the knowledge embeddings, by comparing to natural clusters.\n",
    "    For example, in the example below we cluster the embeddings of international football matches and end up\n",
    "    finding geographical clusters very similar to the continents.\n",
    "    This comparison can be subjective by inspecting a 2D projection of the embedding space or objective using a\n",
    "    `clustering metric <https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation>`_.\n",
    "    | The choice of the clustering algorithm and its corresponding tuning will greatly impact the results.\n",
    "      Please see `scikit-learn documentation <https://scikit-learn.org/stable/modules/clustering.html#clustering>`_\n",
    "      for a list of algorithms, their parameters, and pros and cons.\n",
    "    Clustering is exclusive (i.e. a triple is assigned to one and only one cluster).\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape [n, 3] or [n]\n",
    "        The input to be clustered.\n",
    "        ``X`` can either be the triples of a knowledge graph, its entities, or its relations.\n",
    "        The argument ``mode`` defines whether ``X`` is supposed an array of triples\n",
    "        or an array of either entities or relations.\n",
    "    model : EmbeddingModel\n",
    "        The fitted model that will be used to generate the embeddings.\n",
    "        This model must have been fully trained already, be it directly with\n",
    "        ``fit()`` or from a helper function such as :meth:`ampligraph.evaluation.select_best_model_ranking`.\n",
    "    clustering_algorithm : object\n",
    "        The initialized object of the clustering algorithm.\n",
    "        It should be ready to apply the `fit_predict` method.\n",
    "        Please see: `scikit-learn documentation <https://scikit-learn.org/stable/modules/clustering.html#clustering>`_\n",
    "        to understand the clustering API provided by scikit-learn.\n",
    "        The default clustering model is\n",
    "        `sklearn's DBSCAN <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html>`_\n",
    "        with its default parameters.\n",
    "    mode: string\n",
    "        Clustering mode. Choose from:\n",
    "        - | 'entity' (default): the algorithm will cluster the embeddings of the provided entities.\n",
    "        - | 'relation': the algorithm will cluster the embeddings of the provided relations.\n",
    "        - | 'triple' : the algorithm will cluster the concatenation\n",
    "            of the embeddings of the subject, predicate and object for each triple.\n",
    "    Returns\n",
    "    -------\n",
    "    labels : ndarray, shape [n]\n",
    "        Index of the cluster each triple belongs to.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Note seaborn, matplotlib, adjustText are not AmpliGraph dependencies.\n",
    "    >>> # and must therefore be installed manually as:\n",
    "    >>> #\n",
    "    >>> # $ pip install seaborn matplotlib adjustText\n",
    "    >>>\n",
    "    >>> import requests\n",
    "    >>> import pandas as pd\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.decomposition import PCA\n",
    "    >>> from sklearn.cluster import KMeans\n",
    "    >>> import matplotlib.pyplot as plt\n",
    "    >>> import seaborn as sns\n",
    "    >>>\n",
    "    >>> # adjustText lib: https://github.com/Phlya/adjustText\n",
    "    >>> from adjustText import adjust_text\n",
    "    >>>\n",
    "    >>> from ampligraph.datasets import load_from_csv\n",
    "    >>> from ampligraph.latent_features import ComplEx\n",
    "    >>> from ampligraph.discovery import find_clusters\n",
    "    >>>\n",
    "    >>> # International football matches triples\n",
    "    >>> # See tutorial here to understand how the triples are created from a tabular dataset:\n",
    "    >>> # https://github.com/Accenture/AmpliGraph/blob/master/docs/tutorials/\\\n",
    "ClusteringAndClassificationWithEmbeddings.ipynb\n",
    "    >>> url = 'https://ampligraph.s3-eu-west-1.amazonaws.com/datasets/football.csv'\n",
    "    >>> open('football.csv', 'wb').write(requests.get(url).content)\n",
    "    >>> X = load_from_csv('.', 'football.csv', sep=',')[:, 1:]\n",
    "    >>>\n",
    "    >>> model = ComplEx(batches_count=50,\n",
    "    >>>                 epochs=300,\n",
    "    >>>                 k=100,\n",
    "    >>>                 eta=20,\n",
    "    >>>                 optimizer='adam',\n",
    "    >>>                 optimizer_params={'lr':1e-4},\n",
    "    >>>                 loss='multiclass_nll',\n",
    "    >>>                 regularizer='LP',\n",
    "    >>>                 regularizer_params={'p':3, 'lambda':1e-5},\n",
    "    >>>                 seed=0,\n",
    "    >>>                 verbose=True)\n",
    "    >>> model.fit(X)\n",
    "    >>>\n",
    "    >>> df = pd.DataFrame(X, columns=[\"s\", \"p\", \"o\"])\n",
    "    >>>\n",
    "    >>> teams = np.unique(np.concatenate((df.s[df.s.str.startswith(\"Team\")],\n",
    "    >>>                                   df.o[df.o.str.startswith(\"Team\")])))\n",
    "    >>> team_embeddings = model.get_embeddings(teams, embedding_type='entity')\n",
    "    >>>\n",
    "    >>> embeddings_2d = PCA(n_components=2).fit_transform(np.array([i for i in team_embeddings]))\n",
    "    >>>\n",
    "    >>> # Find clusters of embeddings using KMeans\n",
    "    >>> kmeans = KMeans(n_clusters=6, n_init=100, max_iter=500)\n",
    "    >>> clusters = find_clusters(teams, model, kmeans, mode='entity')\n",
    "    >>>\n",
    "    >>> # Plot results\n",
    "    >>> df = pd.DataFrame({\"teams\": teams, \"clusters\": \"cluster\" + pd.Series(clusters).astype(str),\n",
    "    >>>                    \"embedding1\": embeddings_2d[:, 0], \"embedding2\": embeddings_2d[:, 1]})\n",
    "    >>>\n",
    "    >>> plt.figure(figsize=(10, 10))\n",
    "    >>> plt.title(\"Cluster embeddings\")\n",
    "    >>>\n",
    "    >>> ax = sns.scatterplot(data=df, x=\"embedding1\", y=\"embedding2\", hue=\"clusters\")\n",
    "    >>>\n",
    "    >>> texts = []\n",
    "    >>> for i, point in df.iterrows():\n",
    "    >>>     if np.random.uniform() < 0.1:\n",
    "    >>>         texts.append(plt.text(point['embedding1']+.02, point['embedding2'], str(point['teams'])))\n",
    "    >>> adjust_text(texts)\n",
    "    .. image:: ../../docs/img/clustering/clustered_embeddings_docstring.png\n",
    "    \"\"\"\n",
    "    if not model.is_fitted:\n",
    "        msg = \"Model has not been fitted.\"\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if not hasattr(clustering_algorithm, \"fit_predict\"):\n",
    "        msg = \"Clustering algorithm does not have the `fit_predict` method.\"\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    modes = (\"triple\", \"entity\", \"relation\")\n",
    "    if mode not in modes:\n",
    "        msg = \"Argument `mode` must be one of the following: {}.\".format(\", \".join(modes))\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if mode == \"triple\" and (len(X.shape) != 2 or X.shape[1] != 3):\n",
    "        msg = \"For 'triple' mode the input X must be a matrix with three columns.\"\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if mode in (\"entity\", \"relation\") and len(X.shape) != 1:\n",
    "        msg = \"For 'entity' or 'relation' mode the input X must be an array.\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if mode == \"triple\":\n",
    "        s = model.get_embeddings(X[:, 0], embedding_type='entity')\n",
    "        p = model.get_embeddings(X[:, 1], embedding_type='relation')\n",
    "        o = model.get_embeddings(X[:, 2], embedding_type='entity')\n",
    "        emb = np.hstack((s, p, o))\n",
    "    else:\n",
    "        emb = model.get_embeddings(X, embedding_type=mode)\n",
    "\n",
    "    return clustering_algorithm.fit_predict(emb)\n",
    "\n",
    "\n",
    "def find_duplicates(X, model, mode=\"entity\", metric='l2', tolerance='auto',\n",
    "                    expected_fraction_duplicates=0.1, verbose=False):\n",
    "    r\"\"\"\n",
    "    Find duplicate entities, relations or triples in a graph based on their embeddings.\n",
    "    For example, say you have a movie dataset that was scraped off the web with possible duplicate movies.\n",
    "    The movies in this case are the entities.\n",
    "    Therefore, you would use the 'entity' mode to find all the movies that could de duplicates of each other.\n",
    "    Duplicates are defined as points whose distance in the embedding space are smaller than\n",
    "    some given threshold (called the tolerance).\n",
    "    The tolerance can be defined a priori or be found via an optimisation procedure given\n",
    "    an expected fraction of duplicates. The optimisation algorithm applies a root-finding routine\n",
    "    to find the tolerance that gets to the closest expected fraction. The routine always converges.\n",
    "    Distance is defined by the chosen metric, which by default is the Euclidean distance (L2 norm).\n",
    "    As the distances are calculated on the embedding space,\n",
    "    the embeddings must be meaningful for this routine to work properly.\n",
    "    Therefore, it is suggested to evaluate the embeddings first using a metric such as MRR\n",
    "    before considering applying this method.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray, shape [n, 3] or [n]\n",
    "        The input to be clustered.\n",
    "        X can either be the triples of a knowledge graph, its entities, or its relations.\n",
    "        The argument `mode` defines whether X is supposed an array of triples\n",
    "        or an array of either entities or relations.\n",
    "    model : EmbeddingModel\n",
    "        The fitted model that will be used to generate the embeddings.\n",
    "        This model must have been fully trained already, be it directly with ``fit()``\n",
    "        or from a helper function such as :meth:`ampligraph.evaluation.select_best_model_ranking`.\n",
    "    mode: string\n",
    "        Choose from:\n",
    "        - | 'entity' (default): the algorithm will find duplicates of the provided entities based on their embeddings.\n",
    "        - | 'relation': the algorithm will find duplicates of the provided relations based on their embeddings.\n",
    "        - | 'triple' : the algorithm will find duplicates of the concatenation\n",
    "            of the embeddings of the subject, predicate and object for each provided triple.\n",
    "    metric: str\n",
    "        A distance metric used to compare entity distance in the embedding space.\n",
    "        `See options here <https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html>`_.\n",
    "    tolerance: int or str\n",
    "        Minimum distance (depending on the chosen ``metric``) to define one entity as the duplicate of another.\n",
    "        If 'auto', it will be determined automatically in a way that you get the ``expected_fraction_duplicates``.\n",
    "        The 'auto' option can be much slower than the regular one, as the finding duplicate internal procedure\n",
    "        will be repeated multiple times.\n",
    "    expected_fraction_duplicates: float\n",
    "        Expected fraction of duplicates to be found. It is used only when ``tolerance`` is 'auto'.\n",
    "        Should be between 0 and 1 (default: 0.1).\n",
    "    verbose: bool\n",
    "        Whether to print evaluation messages during optimisation (if ``tolerance`` is 'auto'). Default: False.\n",
    "    Returns\n",
    "    -------\n",
    "    duplicates : set of frozensets\n",
    "        Each entry in the duplicates set is a frozenset containing all entities that were found to be duplicates\n",
    "        according to the metric and tolerance.\n",
    "        Each frozenset will contain at least two entities.\n",
    "    tolerance: float\n",
    "        Tolerance used to find the duplicates (useful in the case of the automatic tolerance option).\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> import numpy as np\n",
    "    >>> import re\n",
    "    >>>\n",
    "    >>> # The IMDB dataset used here is part of the Movies5 dataset found on:\n",
    "    >>> # The Magellan Data Repository (https://sites.google.com/site/anhaidgroup/projects/data)\n",
    "    >>> import requests\n",
    "    >>> url = 'http://pages.cs.wisc.edu/~anhai/data/784_data/movies5.tar.gz'\n",
    "    >>> open('movies5.tar.gz', 'wb').write(requests.get(url).content)\n",
    "    >>> import tarfile\n",
    "    >>> tar = tarfile.open('movies5.tar.gz', \"r:gz\")\n",
    "    >>> tar.extractall()\n",
    "    >>> tar.close()\n",
    "    >>>\n",
    "    >>> # Reading tabular dataset of IMDB movies and filling the missing values\n",
    "    >>> imdb = pd.read_csv(\"movies5/csv_files/imdb.csv\")\n",
    "    >>> imdb[\"directors\"] = imdb[\"directors\"].fillna(\"UnknownDirector\")\n",
    "    >>> imdb[\"actors\"] = imdb[\"actors\"].fillna(\"UnknownActor\")\n",
    "    >>> imdb[\"genre\"] = imdb[\"genre\"].fillna(\"UnknownGenre\")\n",
    "    >>> imdb[\"duration\"] = imdb[\"duration\"].fillna(\"0\")\n",
    "    >>>\n",
    "    >>> # Creating knowledge graph triples from tabular dataset\n",
    "    >>> imdb_triples = []\n",
    "    >>>\n",
    "    >>> for _, row in imdb.iterrows():\n",
    "    >>>     movie_id = \"ID\" + str(row[\"id\"])\n",
    "    >>>     directors = row[\"directors\"].split(\",\")\n",
    "    >>>     actors = row[\"actors\"].split(\",\")\n",
    "    >>>     genres = row[\"genre\"].split(\",\")\n",
    "    >>>     duration = \"Duration\" + str(int(re.sub(\"\\D\", \"\", row[\"duration\"])) // 30)\n",
    "    >>>\n",
    "    >>>     directors_triples = [(movie_id, \"hasDirector\", d) for d in directors]\n",
    "    >>>     actors_triples = [(movie_id, \"hasActor\", a) for a in actors]\n",
    "    >>>     genres_triples = [(movie_id, \"hasGenre\", g) for g in genres]\n",
    "    >>>     duration_triple = (movie_id, \"hasDuration\", duration)\n",
    "    >>>\n",
    "    >>>     imdb_triples.extend(directors_triples)\n",
    "    >>>     imdb_triples.extend(actors_triples)\n",
    "    >>>     imdb_triples.extend(genres_triples)\n",
    "    >>>     imdb_triples.append(duration_triple)\n",
    "    >>>\n",
    "    >>> # Training knowledge graph embedding with ComplEx model\n",
    "    >>> from ampligraph.latent_features import ComplEx\n",
    "    >>>\n",
    "    >>> model = ComplEx(batches_count=10,\n",
    "    >>>                 seed=0,\n",
    "    >>>                 epochs=200,\n",
    "    >>>                 k=150,\n",
    "    >>>                 eta=5,\n",
    "    >>>                 optimizer='adam',\n",
    "    >>>                 optimizer_params={'lr':1e-3},\n",
    "    >>>                 loss='multiclass_nll',\n",
    "    >>>                 regularizer='LP',\n",
    "    >>>                 regularizer_params={'p':3, 'lambda':1e-5},\n",
    "    >>>                 verbose=True)\n",
    "    >>>\n",
    "    >>> imdb_triples = np.array(imdb_triples)\n",
    "    >>> model.fit(imdb_triples)\n",
    "    >>>\n",
    "    >>> # Finding duplicates movies (entities)\n",
    "    >>> from ampligraph.discovery import find_duplicates\n",
    "    >>>\n",
    "    >>> entities = np.unique(imdb_triples[:, 0])\n",
    "    >>> dups, _ = find_duplicates(entities, model, mode='entity', tolerance=0.4)\n",
    "    >>> print(list(dups)[:3])\n",
    "    [frozenset({'ID4048', 'ID4049'}), frozenset({'ID5994', 'ID5993'}), frozenset({'ID6447', 'ID6448'})]\n",
    "    >>> print(imdb[imdb.id.isin((4048, 4049, 5994, 5993, 6447, 6448))][['movie_name', 'year']])\n",
    "                        movie_name  year\n",
    "    4048          Ulterior Motives  1993\n",
    "    4049          Ulterior Motives  1993\n",
    "    5993          Chinese Hercules  1973\n",
    "    5994          Chinese Hercules  1973\n",
    "    6447  The Stranglers of Bombay  1959\n",
    "    6448  The Stranglers of Bombay  1959\n",
    "    \"\"\"\n",
    "\n",
    "    if not model.is_fitted:\n",
    "        msg = \"Model has not been fitted.\"\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    modes = (\"triple\", \"entity\", \"relation\")\n",
    "    if mode not in modes:\n",
    "        msg = \"Argument `mode` must be one of the following: {}.\".format(\", \".join(modes))\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if mode == \"triple\" and (len(X.shape) != 2 or X.shape[1] != 3):\n",
    "        msg = \"For 'triple' mode the input X must be a matrix with three columns.\"\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if mode in (\"entity\", \"relation\") and len(X.shape) != 1:\n",
    "        msg = \"For 'entity' or 'relation' mode the input X must be an array.\"\n",
    "        logger.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if mode == \"triple\":\n",
    "        s = model.get_embeddings(X[:, 0], embedding_type='entity')\n",
    "        p = model.get_embeddings(X[:, 1], embedding_type='relation')\n",
    "        o = model.get_embeddings(X[:, 2], embedding_type='entity')\n",
    "        emb = np.hstack((s, p, o))\n",
    "    else:\n",
    "        emb = model.get_embeddings(X, embedding_type=mode)\n",
    "\n",
    "    def get_dups(tol):\n",
    "        \"\"\"\n",
    "         Given tolerance, finds duplicate entities in a graph based on their embeddings.\n",
    "         Parameters\n",
    "         ----------\n",
    "         tol: float\n",
    "             Minimum distance (depending on the chosen metric) to define one entity as the duplicate of another.\n",
    "         Returns\n",
    "         -------\n",
    "         duplicates : set of frozensets\n",
    "             Each entry in the duplicates set is a frozenset containing all entities that were found to be duplicates\n",
    "             according to the metric and tolerance.\n",
    "             Each frozenset will contain at least two entities.\n",
    "        \"\"\"\n",
    "        nn = NearestNeighbors(metric=metric, radius=tol)\n",
    "        nn.fit(emb)\n",
    "        neighbors = nn.radius_neighbors(emb)[1]\n",
    "        idx_dups = ((i, row) for i, row in enumerate(neighbors) if len(row) > 1)\n",
    "        if mode == \"triple\":\n",
    "            dups = {frozenset(tuple(X[idx]) for idx in row) for i, row in idx_dups}\n",
    "        else:\n",
    "            dups = {frozenset(X[idx] for idx in row) for i, row in idx_dups}\n",
    "        return dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a5bb34bffb12c49171c6b9bf7f67eeb6f0b169a0a68257c4a256d0a5c373a53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
